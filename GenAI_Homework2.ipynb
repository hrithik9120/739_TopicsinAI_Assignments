{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrithik9120/739_TopicsinAI_Assignments/blob/main/GenAI_Homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ndmbCYfECXm"
      },
      "source": [
        "Copy file then ADD YOUR NAME(S) HERE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XaCt2iAECXo"
      },
      "outputs": [],
      "source": [
        "ALERT: before starting this homework, you need to add us (the grader and instructor) to your Weights and Biases accounts so that we can check your previous homework. So please do the following.\n",
        "\n",
        "Open the following link:\n",
        "\n",
        "https://wandb.ai/YOURNAME-rochester-institute-of-technology/members\n",
        "\n",
        "Where YOURNAME is your username.\n",
        "\n",
        "Next, add the following people as users to your team:\n",
        "christopher-m-homan-phd\n",
        "ayowolabi7\n",
        "\n",
        "10 points: users successfully added"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "QJKCqB9tnwbb"
      },
      "source": [
        "The primary goal of this homework is to learn the basics of language models, **before** delving into neural models.\n",
        "A secondary goal is to acquaint you with the **pandas** library.\n",
        "\n",
        "In this homework, you will take a partially coded workbook for a **naive baysian language model**, built using **only** the pandas library.\n",
        "\n",
        "Pandas is **the** standard python libary for working with data, and since GenAI is very much about using data, it will be beneficial for you to have familiarity with it. It is based on **dataframe** interface, similar to a spreadsheet, though closer to an R dataframe. Pandas has extensive documentation. We recommend you use it. It particular, familiarize yourself with .loc and .iloc; they are the main retrieval (and update) methods. There are a lot of useful helper functions. We hope to introduce you to many of them in this code, and to challenge you to use the documentation to figure out others. We recommend you turn off gemini or any other AI assistance for this lab, as it will help your comprehension to do as much of this work yourself, if you can.\n",
        "\n",
        "Please copy this worksheet and read through it. There are a number of tasks for you to perform for points. (Points are indicated.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DMqi7vKECXp"
      },
      "source": [
        "Load libraries. **DO NOT CHANGE** You may may not use any other libraries for this project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcScEjK3mKgA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk9zDj3Fn8Da"
      },
      "source": [
        "Tokenize data. This is an extremely simple process that uses python's built in split command to take anything separated by whitespace ass a character. The first function adds spaces between special characters of interest so that they can be easily tokenized using split. The next is the main tokenization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vH9qGNzoRqQ"
      },
      "outputs": [],
      "source": [
        "def stringout(x, c):\n",
        "  \"\"\"\n",
        "  Pad out a special character with spaces so that it can be treated as a token.\n",
        "\n",
        "  Args:\n",
        "    x: A character string to be tokenized.\n",
        "\n",
        "    c: A character to be padded with spaces.\n",
        "\n",
        "  Returns:\n",
        "    The character string x with spaces added before and after each instance of\n",
        "    c.\n",
        "  \"\"\"\n",
        "  return x.replace(c, \" \" + c + \" \")\n",
        "\n",
        "def tokenize(x):\n",
        "  \"\"\"\n",
        "  Simple tokenization using the str.split() function\n",
        "\n",
        "  Args:\n",
        "    x: A character string to be tokenized:\n",
        "\n",
        "  Returns:\n",
        "    A list of tokens. This will be converted into a dataframe.\n",
        "  \"\"\"\n",
        "  for c in [\",\", \":\", \";\", \".\", \"!\", \"?\", \"\\\"\", \"\\'\", \"(\", \")\"]:\n",
        "    x = stringout(x, c)\n",
        "  return x.split()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juceQye1rKqP"
      },
      "source": [
        "Create dataframe of word (unigram) counts. This will be the basis of later ngram models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhsSd7KdpX0R"
      },
      "outputs": [],
      "source": [
        "def word_count_dataframe(ls_df):\n",
        "  \"\"\"\n",
        "  convert a list of words into a dataframe of word counts\n",
        "\n",
        "  Args:  # You may add more arguments if you like\n",
        "    ls_df: a dataframe of tokens, with one column of tokens and one token per row. Row order is significant.\n",
        "\n",
        "  Returns:\n",
        "    A dataframe of word counts based on the tokens in the list.\n",
        "  \"\"\"\n",
        "\n",
        "  ls_counts_df = pd.DataFrame(ls_df.value_counts())\n",
        "  ls_counts_df.index = [\" \".join(x) for x in ls_counts_df.index.to_flat_index()]\n",
        "\n",
        "  # These are simple rules for determining what to keep in your vocabulary and what you want to leave you.\n",
        "  # YOU CAN CHANGE THESE AS YOU SEE FIT\n",
        "  ls_counts_oov = ls_counts_df[ls_counts_df[\"count\"] <= 1]\n",
        "  sum = int(ls_counts_oov.sum())\n",
        "\n",
        "  ls_counts_keep =  ls_counts_df[ls_counts_df[\"count\"] > 1]\n",
        "\n",
        "  # Add an entry for the counts of out-of-vocabulary (OOV) words. This is an example of the kind of accomodations that need to be made for\n",
        "  # OOVs throughout this notebook.\n",
        "  # This is a common but seemingly uncoventional way to add a row to a dataframe. It will throw a runtime warning but it is safe\n",
        "  # to do this here.\n",
        "  ls_counts_keep.loc['OOV'] = {'count': sum}\n",
        "\n",
        "  return ls_counts_keep\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN8pJetqjhWY"
      },
      "source": [
        "This cell shows how to create a bigram model. You can use this to do bigram generation (or unicode generation). You will be asked to generalize this approach from a bigram to an ngram model for any n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqGnDhIScbRC"
      },
      "outputs": [],
      "source": [
        "def create_bigram_model(words_df):\n",
        "    \"\"\"Construct a dataframe of a bigram model\n",
        "\n",
        "    Args:\n",
        "        words_df: a list of tokens, representing a body of text\n",
        "\n",
        "    Returns:\n",
        "        A dataframe of bigrams\n",
        "    \"\"\"\n",
        "    words1_df = words_df\n",
        "    words1_df.columns = [\"token1\"]\n",
        "    words2_df = words_df.copy()\n",
        "    words2_df.columns = [\"token2\"]\n",
        "    words2_df.index = words1_df.index - 1\n",
        "\n",
        "    bigram_df = pd.merge(left_index=True, right_index=True, left=words1_df, right=words2_df, how=\"inner\")\n",
        "    return bigram_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEpBN-20ECXq"
      },
      "source": [
        "You need to write the next couple of functions. You may include helper functions in each cell. Here is what the n-gram table should look like-format-wise (note that the counts are not realistic).\n",
        "\n",
        "\n",
        "      token1 token2.   token3 token4 count\n",
        "    0 am     drawn\t   in     .      482\n",
        "    1 am     down      again  ;      451\n",
        "    2 am     doubtless I      can    411\n",
        "    3 for    my        grief  so     382"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR3wCxq8ECXq"
      },
      "outputs": [],
      "source": [
        "def create_ngram_df(words_df, n):\n",
        "    \"\"\" Generate a dataframe of ngrams.\n",
        "\n",
        "    Args:\n",
        "        words_df: A dataframe with a single column, with a token in each row, representing text data.\n",
        "        n: the maximum number of grams.\n",
        "\n",
        "    Returns:\n",
        "        An ngram table with one column for each gram, labeled token1 token2.. tokenn and one row for each n gram\n",
        "        in words_df, in order.\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKQsQQt2ECXq"
      },
      "source": [
        "To generate next character counts, first write code that can handle the bigram model, then get it to handle arbitrary ngrams. This should look up ngrams in the model, then sample the next character generate. To choose the next character, call the Pandas DataFrame method sample, using the argument \"weights\" to sample the next character randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SmHJdsuiECXq"
      },
      "outputs": [],
      "source": [
        "def generate_from_text(s, n, model, m, random=True):\n",
        "    \"\"\"Generate a string of input based on a string prompt.\n",
        "\n",
        "    Parses the string s into a list of words and then queries the model to determine the next word\n",
        "\n",
        "    s: A string representing a prompt. Use and the model to generate the string.\n",
        "    n: Size of ngrams to use. N-1 grams are used for prediction and the nth gram is used for prediction.\n",
        "    model: An ngram model, consisting of 1 row per n-gram, one column per token and a counts column indicating the (relative) frequency\n",
        "        of that ngram\n",
        "    m: The number of new tokens to generate\n",
        "    random: If True, sample next token randomly according to its token count. Otherwise, choose the most likely ngram.\n",
        "\n",
        "    Returns:\n",
        "        A string contains the prompt and response, as a space-separated list of tokens.\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxgZyWGuECXq"
      },
      "source": [
        "Load data files. These are available at:\n",
        "\n",
        "https://www.gutenberg.org/cache/epub/2701/pg2701.txt\n",
        "https://www.gutenberg.org/cache/epub/100/pg100.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1THew4TnaLO"
      },
      "outputs": [],
      "source": [
        "f = open(\"mobydick.txt\")\n",
        "moby = f.readlines()\n",
        "f.close()\n",
        "\n",
        "f = open(\"shakespeare.txt\")\n",
        "shakes = f.readlines()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xSdDbzqECXr"
      },
      "source": [
        "Both of these files have a lot of things in them that are not useful, like unnecessary punctuation and capitalization, and the beginning and ending matter from project Gutenberg. Use this cell to remove such other clean the data (you can add additional cells as needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3tUM0r5ECXr"
      },
      "outputs": [],
      "source": [
        "new_moby = moby\n",
        "new_shakes = shakes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsVlepHXECXr"
      },
      "source": [
        "This cell saves the cleaned data into new files. Once you clean the data, you can skip the previous cells involving the old files (comment them out so that we can see what you did to fix them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZUtG3iuECXr"
      },
      "outputs": [],
      "source": [
        "moby = \" \".join(new_moby)\n",
        "f = open(\"mobydick_cleaned.txt\", \"w\")\n",
        "f.writelines(moby)\n",
        "f.close()\n",
        "\n",
        "shakes = \" \".join(new_shakes)\n",
        "f = open(\"shakespeare_cleaned.txt\", \"w\")\n",
        "f.writelines(shakes)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05mVJGyabbnn"
      },
      "source": [
        "Set up basic bayesian (unicode) model. We can use word counts to generate texts based on sampling from word_counts_df. We can also use this\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EdNP-QevSHV",
        "outputId": "c7b5b038-3e2d-431d-fa7e-e7eeaae55844"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1y/gk13cyp11r57csry99m9htmr0000gs/T/ipykernel_53575/3397008202.py:18: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  sum = int(ls_counts_oov.sum())\n",
            "/var/folders/1y/gk13cyp11r57csry99m9htmr0000gs/T/ipykernel_53575/3397008202.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ls_counts_keep.loc['OOV'] = {'count': sum}\n"
          ]
        }
      ],
      "source": [
        "moby = tokenize(moby)\n",
        "shakes = tokenize(shakes)\n",
        "\n",
        "moby_df = pd.DataFrame(moby, columns = [\"tokens\"])\n",
        "shakes_df = pd.DataFrame(shakes, columns = [\"tokens\"])\n",
        "\n",
        "words_df = pd.concat([moby_df, shakes_df], ignore_index=True).reindex()\n",
        "word_counts_df = word_count_dataframe(words_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0628-h1ECXr"
      },
      "source": [
        "Set up and run experiments on bigram model. Your main goal is to generalize this to an ngram model and run similar tests on different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyGvi-lCECXr",
        "outputId": "483818c7-b8c5-41c4-fa76-b429cb1f548a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token1</th>\n",
              "      <th>token2</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "      <td>10329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "      <td>I</td>\n",
              "      <td>4906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>,</td>\n",
              "      <td>I</td>\n",
              "      <td>4557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>,</td>\n",
              "      <td>And</td>\n",
              "      <td>4321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>3544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444763</th>\n",
              "      <td>hebenon</td>\n",
              "      <td>in</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444764</th>\n",
              "      <td>heav’n</td>\n",
              "      <td>he</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444765</th>\n",
              "      <td>heav’n</td>\n",
              "      <td>,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444766</th>\n",
              "      <td>heav’d</td>\n",
              "      <td>to</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444767</th>\n",
              "      <td>her</td>\n",
              "      <td>Indian</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>444768 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         token1  token2  count\n",
              "0             ,     and  10329\n",
              "1             .       I   4906\n",
              "2             ,       I   4557\n",
              "3             ,     And   4321\n",
              "4            of     the   3544\n",
              "...         ...     ...    ...\n",
              "444763  hebenon      in      1\n",
              "444764   heav’n      he      1\n",
              "444765   heav’n       ,      1\n",
              "444766   heav’d      to      1\n",
              "444767      her  Indian      1\n",
              "\n",
              "[444768 rows x 3 columns]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigrams_df = create_bigram_model(words_df)\n",
        "bigrams_df\n",
        "bigram_counts_df = bigrams_df.value_counts().reset_index()\n",
        "bigram_counts_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDgMJG6qECXr"
      },
      "source": [
        "Use these queries for all of the tests below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9jVJqeiECXr"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"bare head was just level\",\n",
        "    \"Had you rather Caesar were living, and die all slaves\",\n",
        "    \"MERCURIO: The apple\",\n",
        "    \"Warm up the computer and aim the space laser\",\n",
        "    \"Blasted whale !\",\n",
        "    \"Call me Ishmael\",\n",
        "    \"Don't call me Ishmael\",\n",
        "    \"Call me CAESAR\",\n",
        "    \"QUESTION : What is two plus two ? ANSWER :\",\n",
        "    \"QUESTION : From which direction rises the sun ? ANSWER :\",\n",
        "    \"QUESTION : How dost one wield a harpoon ? ANSWER :\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZaDlj6MECXr"
      },
      "source": [
        "Your code should run to completion up to this point (once you download the data). The code below will NOT run, until you fix things.\n",
        "\n",
        "5 points: write code for the function \"generate from text\" so that it writes responses to most of the queries.\n",
        "10 point (total):  write code for the function \"generate from text\" so that it writes responses to all of the queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO92wkcmECXr",
        "outputId": "3f8af7ca-75f3-42fd-f4ff-2de22f8a0976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bare head was just level on water , and from the Dukes of the weary watch . I will bestow you shall it in the last so much believed , Grumio , ” Anon expect my body to strike at his tent , and depart . Westminster , moodily unaccounted for Marina at him by whose reverberations , Which shall kingly seal , though he will mourn for you are hard to redder drops That you been your leisure to obtain my spirit ? and the near me . Enter Leontes shall ere this speedy enterprise Of dreaded creatures , That fair ? — Quohog\n",
            "\n",
            "Had you rather Caesar were living , and die all slaves With coral beach , the mind . Say this green , and all , not to terrify and base ? POMPEY . With who died the sight . WARWICK . PRINCE OF SYRACUSE . It is knocked into the fore part of the safety of love itself must be kind Prince That , I thank your winking of logs , poor Anne , which had been beyond also defective in excrements , madam , Congreeing in or unsafe lunes again ? O , that which the side to a simple souls to nothing To say but thee To give me\n",
            "\n",
            "MERCURIO : The apple . SECOND FRIEND . ALL . [_Exit Catesby . KING . Our chiefest friend of all these were as is deposed ? MONTGOMERY . BRUTUS . NURSE . Enter Dromio of Syracuse . He rests : so please you know him to his loss , it ? Truth shake thy master , and certainly thou learn her ! what cry hem ! [_Exeunt . I were living leave her father was created both against the benefit . O ! OLIVIA . The life be angry brow his lady’s brows . No , they were buried in clouds . I am\n",
            "\n",
            "Warm up the computer and aim the space laser that . The wood , Which is the mire . Am I spoke to ride , the diminished ; go stand by me embrace with a thatched house ; And yet the weight of a place , sir . O , or something too , She says all , marry , Thou , But as much treasure of Arcite . My sinful father in the ground : there’s no candles are Despised in which taken a day arising From every one more to it after a tree with rage and fast . No evil that it , I have won\n",
            "\n",
            "Blasted whale ! GLOUCESTER . What work for you descend with the chain these There falling in faith . SLENDER . Ay , methinks , good that , if I am an eBook , is the softest touch this dead , Lest by eight kings to help of a terrible impression bears down . First told me ! You seemed that begg’d mine own foes—deliver you his rifle from a hope your penance . ’Tis his throne , declares , would be here , poor brother’s right under one of the whale’s spout-hole ; but to a just been held down in praises\n",
            "\n",
            "Call me Ishmael , but two pledges of war , lad . No , Duke , divorce not us . [_To the like a thing to thrive , Shall yield to sweeten Of both . _She loves you . Flourish . It was given me then appointed for them ; naught from the same dog-fox , Which , as you further , that he be sure as you for I for the summer dead many miles and the banish’d moody secrets That only to speak Before we , spinning that the worst Is strict to be true , yet thou canst not to\n",
            "\n",
            "Don ' t call me Ishmael . Enter Helena . Thou disease , a thing . BEROWNE . From Claudio promis’d it , my lords , I know , I’ll give us . A dateless lively cry out to prove She’s born of it , go with the cure . O gull , fly in haste . Pray you now as God pardon ! BENVOLIO . Wert thou turn into the island as one a very wisely in Nantucket grimness was decreed , Thy two of those that what dost thou art a piece of heartbreak . True , Torturing convulsions from his . If we\n",
            "\n",
            "Call me CAESAR . CHAPTER 28 . AUTOLYCUS . Even as will ever threat , like lightning ? [_Beating him once she blows , And thou shalt wane ; calmly rose of his rage might well cut off his company . Alas , Methought I did use your worship . ” said enough , being a promis’d to a greater harm , why dost thou seest it so suddenly performed , For the victory , feel this speedy scouts , Which enter’d their papers were pity ? PRINCE . How ? THIRD CITIZEN . BRUTUS . Not wounding . Doubtless the one .\n",
            "\n",
            "QUESTION : What is two plus two ? ANSWER : Which humbleness , my heart torment to those already . You moonshine , to Titus above water ; but a gentleman— Think you a child of fever that we put his head embraces one so did he stands on you truly , my honourable gentleman . DUKE . Nay , verses of the surface remain on the other wild As for whales , sir ; bunched whales , To dog bark ( but slightly bowing your face of his friend , During all other creature . VIRGILIA . Heaven hath been watching . Between our delight , and what cates\n",
            "\n",
            "QUESTION : From which direction rises the sun ? ANSWER : If a kinsman . [_Dies . _] Signior Brabantio . Wherefore hast thou unsalted leaven , but by the stomach . Saint Edmundsbury ; let alone . , perseverance , Sir Gilbert Talbot , Beauty doth extenuate ) , who , Empress I : Though forfeiters you ; fill the sound the body . Pure shame bids her , but to shake That thou entreat your rest in the Bachelor ; for all the pursuit ; he hath not without his not ever have I get my nation shall obey necessity in . [_Exit . With good bringing up and\n",
            "\n",
            "QUESTION : How dost one wield a harpoon ? ANSWER : Diana’s temple . It is marvellous features of his business , of war . CHAPTER 129 . [_He stamps as immediate freedom unto him death . I wonder . Good pilgrim is thus , follows our dear niece ; and degree , slain , From year , Tu-whoo ! Thou then again growled , and wring under sail . Thou show’dst the back-trick simply as the air breathes out . Come , others the lawyers , hang her eye , the bolting another time o’ th’ contrary ? I ready . Now , Whose frothy mouth . A . Enter\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 2, bigram_counts_df, 100))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kIJLzjHECXr"
      },
      "source": [
        "Now, rather than randomly generating the code, modify generate_from_text so that it will also select the most likely next token. If two or more tokens are most likely, any of them may be chosen.\n",
        "\n",
        "8 points: write code for the function \"generate from text\" so that it writes responses to most of the queries.\n",
        "10 points (total):  write code for the function \"generate from text\" so that it writes responses to all of the queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdFelt7BECXs"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 2, bigram_counts_df, 100, random=False))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps7utELzECXs"
      },
      "source": [
        "Now, create two new bigram models, one from the shakespeare data ONLY, and one from moby-dick ONLY. Call them shakes_bigrams_counts_df and moby_bigram_counts_df respectively (create new cells as needed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vkn16lAECXs"
      },
      "source": [
        "Run tests on the new models.\n",
        "17 points: write code for the function \"generate from text\" so that it writes responses to most of the queries.\n",
        "20 points (total):  write code for the function \"generate from text\" so that it writes responses to all of the queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj7Q_TUaECXs"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 2, moby_bigram_counts_df, 100))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXYKKgIwECXs"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 2, moby_bigram_counts_df, 100, random=False))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtGhS1SFECXs"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 2, shakes_bigram_counts_df, 100))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hnm4NclECXs"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 2, shakes_bigram_counts_df, 100, random=False))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IARNsg_eECXs"
      },
      "source": [
        "Now, implement create_ngram_df to create ngram models for arbitrarily large ngrams. Create three models: moby_ngram_counts_df, shakes_ngram_counts_df and ngram_counts_df, for, respectively moby-dick only, shakespeare only, and both texts. These models must at least produce 4-grams (create new cells as needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXmWbg8vECXs"
      },
      "outputs": [],
      "source": [
        "Run tests on the new models.\n",
        "36 points: write code for the function \"generate from text\" so that it writes responses to most of the queries.\n",
        "40 points (total):  write code for the function \"generate from text\" so that it writes responses to all of the queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDNa9vsRECX1"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 4, moby_ngram_counts_df, 100))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLVoPccCECX1"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 4, moby_ngram_counts_df, 100, random=False))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUUsN7FVECX1"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 4, shakes_ngram_counts_df, 100))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzVXlVGgECX1"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 4, shakes_ngram_counts_df, 100, random=False))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5LUef-mECX1"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 4, ngram_counts_df, 100))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LMPJ7vVECX1"
      },
      "outputs": [],
      "source": [
        "for query in queries:\n",
        "    print(generate_from_text(query, 4, ngram_counts_df, 100, random=False))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LcWpvrzECX1"
      },
      "source": [
        "Finally, the bigram_counts_df models uses columns to denotes each of the \"grams.\" You should take a similar approach in your first pass of the *ngram_counts_df models. However, this approach is slow, and so once you get your code working with columns, you should look into storing the ngrams as a multi-index.\n",
        "\n",
        "10 points: Code uses multindices, rather than columns, to store and lookup \"grams.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFjTIDljECX1"
      },
      "source": [
        "SUBMITTING. Save as a .pynb file and submit that file via mycourses."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}